# -*- coding: utf-8 -*
'''
åŒ¹é…contentã€commentå¹¶åˆæ­¥æ¸…ç†æ•°æ®
ç”¨äºä½œè¯äº‘wc.pyã€åœ°å›¾graph.pyã€Tf-idfæ–‡æœ¬èšç±»cluster_tfidfã€Word2Vecæ–‡æœ¬èšç±»cluster_w2v
å°†æ¯ä¸ªåˆ†ç±»å…³é”®è¯å½¢å¼å˜ä¸ºï¼š
[
  [url1, content1åŸæ–‡, [content1åˆ†è¯],[comment1],...,[comment_n]],
  [url2, content2åŸæ–‡, [content2åˆ†è¯],[comment1],...,[comment_m]],
  ...
]
Match content and comment and initially clean up the data
Used to make word cloud wc.py, map graph.py, Tf-idf text clustering cluster_tfidf, Word2Vec text clustering cluster_w2v
Change the form of each category keyword to 
[
  [url1, content1åŸæ–‡, [content1åˆ†è¯],[comment1],...,[comment_n]],
  [url2, content2åŸæ–‡, [content2åˆ†è¯],[comment1],...,[comment_m]],
  ...
]
'''
import json
import pandas as pd
import jieba
import pickle
import re

from dict import langconv


def Traditional2Simplified(sentence):
    '''
    Convert traditional characters in sentence to simplified characters
    :param sentence: sentence to be converted
    :return: Convert the traditional characters in the sentence to the sentence after the simplified characters
    '''
    sentence = langconv.Converter('zh-hans').convert(sentence)
    return sentence

def Sent2Word(sentence):
    """Turn a sentence into tokenized word list and remove stop-word

    Using jieba to tokenize Chinese.

    Args:
        sentence: A string.

    Returns:
        words: A tokenized word list.
    """
    global stop_words

    words = jieba.cut(sentence)
    words = [w for w in words if w not in stop_words]

    return words


def Match(content):
    """åŒ¹é…å¾®åšå†…å®¹å’Œå¾®åšè¯„è®ºæ•°æ®ï¼Œå¹¶å°†æ˜æ˜¾çš„å¹¿å‘Šå¾®åšå‰”é™¤
    Match Weibo content and Weibo comment data, and eliminate obvious advertising Weibo

    Args:
        comment_example:
      [
      {'_id': 'C_4322161898716112', 'crawl_time': '2019-06-01 20:35:36', 'weibo_url': 'https://weibo.com/1896820725/H9inNf22b', 'comment_user_id': '6044625121', 'content': 'æ²¡é—®é¢˜ï¼Œ', 'like_num': {'$numberInt': '0'}, 'created_at': '2018-12-28 11:19:21'},...
      ]

        content_example:
      [
      {'_id': '1177737142_H4PSVeZWD', 'keyword': 'Aè‚¡', 'crawl_time': '2019-06-01 20:31:13', 'weibo_url': 'https://weibo.com/1177737142/H4PSVeZWD', 'user_id': '1177737142', 'created_at': '2018-11-29 03:02:30', 'tool': 'Android', 'like_num': {'$numberInt': '0'}, 'repost_num': {'$numberInt': '0'}, 'comment_num': {'$numberInt': '0'}, 'image_url': 'http://wx4.sinaimg.cn/wap180/4632d7b6ly1fxod61wktyj20u00m8ahf.jpg', 'content': '#aè‚¡è§‚ç‚¹# é²å¨å°”ä¸»å¸­æˆ–æ˜¯å› ä¸ºè¢«ç‰¹æœ—æ™®æ€»ç»Ÿç‚¹åæ‰¹è¯„åèŒç”Ÿæ‚”æ”¹ä¹‹æ„ï¼Œä»Šæ™šä¸€ç•ªè®²è¯è¢«å¸‚åœºè§£è¯»ä¸ºç¾è”å‚¨æˆ–æš‚åœåŠ æ¯æ­¥ä¼ã€‚ç¾å…ƒæŒ‡æ•°åº”å£°ä¸‹æŒ«ï¼Œç¾è‚¡åŠé‡‘å±è´µé‡‘å±ä»·æ ¼å¤§å¹…ä¸Šæ‰¬ï¼ŒA50è¡¨ç°ä¹Ÿå¹¶ä¸é€Šè‰²å¤ªå¤šã€‚å¯¹æ˜å¤©Aè‚¡æˆ–æœ‰ç§¯æå½±å“ï¼Œåå¼¹æˆ–èƒ½å¾—ä»¥å»¶ç»­ã€‚ [ç»„å›¾å…±2å¼ ]'},...
      ]

    Returns:
        å…¶å®æ²¡æœ‰returnï¼Œå½¢æˆå¦‚ä¸‹æ ¼å¼çš„pklæ–‡ä»¶ï¼š
        [
        [url1, content1åŸæ–‡, [content1åˆ†è¯],[comment1],...,[comment_n]],
        [url2, content2åŸæ–‡, [content2åˆ†è¯],[comment1],...,[comment_m]],
        ...
        ]
    """
    content_comment = []
    advertisement = ["ç‹è€…è£è€€", "åˆ¸å", "å”®ä»·", 'Â¥', "ï¿¥", 'ä¸‹å•']

    for k in range(0, len(content)):
        judge = []
        print('Processing train ', k)
        print(content[k]['content'])
        content[k]['content'] = Traditional2Simplified(content[k]['content'])
        for adv in advertisement:
            if adv in content[k]['content']:
                judge.append("True")
                break
        if re.search(r"ä¹°.*èµ .*", content[k]['content']):
            judge.append("True")
            continue
        # é€šè¿‡ä¸Šé¢çš„ä¸¤ç§æ¨¡å¼åˆ¤æ–­æ˜¯ä¸æ˜¯å¹¿å‘Š
        # Determine whether it is an advertisement through the above two modes
        if "True" not in judge:
            comment_list = []
            comment_list.append(content[k]['content'])
            # æ•°æ®æ¸…æ´—
            a2 = re.compile(r'#.*?#')
            content[k]['content'] = a2.sub('', content[k]['content'])
            a3 = re.compile(r'\[ç»„å›¾å…±.*å¼ \]')
            content[k]['content'] = a3.sub('', content[k]['content'])
            a4 = re.compile(r'http:.*')
            content[k]['content'] = a4.sub('', content[k]['content'])
            a5 = re.compile(r'@.*? ')
            content[k]['content'] = a5.sub('', content[k]['content'])
            a6 = re.compile(r'\[.*?\]')
            content[k]['content'] = a6.sub('', content[k]['content'])
            comment_list.append(Sent2Word(content[k]['content']))
            content_comment.append(comment_list)

    pickle.dump(content_comment, open('Agu.pkl', 'wb'))


if __name__ == '__main__':

    print("åœç”¨è¯è¯»å–")
    stop_words = [w.strip() for w in open('dict/å“ˆå·¥å¤§åœç”¨è¯è¡¨.txt', 'r', encoding='UTF-8').readlines()]
    stop_words.extend(['\n', '\t', ' ', 'å›å¤', 'è½¬å‘å¾®åš', 'è½¬å‘', 'å¾®åš', 'ç§’æ‹', 'ç§’æ‹è§†é¢‘', 'è§†é¢‘', "ç‹è€…è£è€€", "ç‹è€…", "è£è€€"])
    for i in range(128000, 128722 + 1):
        stop_words.extend(chr(i))
    stop_words.extend(['Aè‚¡'])

    # ç‰¹æ®Šå­—ç¬¦

    # print(ord("ğŸ"))
    # print(chr(77823))
    # print(hex(128300))
    # print(0x1E000)
    # # E000-F8FF 122880-129279  # è‡ªè¡Œä½¿ç”¨å€åŸŸ (Private Use Zone)
    # # F900-FAFF 129280-129791
    # count = 0
    # for i in range(128000, 129279+1):
    #     count += 1
    #     if count <100:
    #         print(chr(i))
    #     else:
    #         break

    #  jsonæ–‡æ¡£è¯»æ³•
    # print("commentè¯»å–")
    # f = codecs.open('./Agu_comment.json', 'r', 'UTF-8-sig')
    # comment = []
    # for i in f.readlines():
    #     try:
    #         comment.append(eval(i))
    #     except:
    #         continue
    # # comment = [json.loads(i) for i in f.readlines()]  # json.loadsä¹Ÿè¡Œ
    # f.close()
    # # print(comment)

    '''
      comment_example:
      [
      {'_id': 'C_4322161898716112', 'crawl_time': '2019-06-01 20:35:36', 'weibo_url': 'https://weibo.com/1896820725/H9inNf22b', 'comment_user_id': '6044625121', 'content': 'æ²¡é—®é¢˜ï¼Œ', 'like_num': {'$numberInt': '0'}, 'created_at': '2018-12-28 11:19:21'},...
      ]
    '''

    # pandas csvæ–‡æ¡£è¯»æ³•
    df_weibo = pd.read_csv(r'/Users/chz/Downloads/weibo/wei/weibo_analysis_and_visualization/æ–°å† ç–«è‹—.csv', sep=',', quotechar='"', error_bad_lines=False,encoding='gbk')
    #print(df_weibo.head())
    df_weibo = df_weibo.to_json(orient="records")
    df_weibo = json.loads(df_weibo)
    print(type(df_weibo))

    # print("contentè¯»å–")
    # f = codecs.open('./Agu_content1.json', 'r', 'UTF-8-sig')
    # content = []
    # for i in df_weibo:
    #     print(i)
    #     try:
    #         content.append(eval(i))
    #     except:
    #         continue
    # # jinkou = [json.loads(i) for i in f.readlines()]  # json.loadsä¹Ÿè¡Œ
    # f.close()
    # # print(content)

    '''
      content_example:
      [
      {'_id': '1177737142_H4PSVeZWD', 'keyword': 'Aè‚¡', 'crawl_time': '2019-06-01 20:31:13', 'weibo_url': 'https://weibo.com/1177737142/H4PSVeZWD', 'user_id': '1177737142', 'created_at': '2018-11-29 03:02:30', 'tool': 'Android', 'like_num': {'$numberInt': '0'}, 'repost_num': {'$numberInt': '0'}, 'comment_num': {'$numberInt': '0'}, 'image_url': 'http://wx4.sinaimg.cn/wap180/4632d7b6ly1fxod61wktyj20u00m8ahf.jpg', 'content': '#aè‚¡è§‚ç‚¹# é²å¨å°”ä¸»å¸­æˆ–æ˜¯å› ä¸ºè¢«ç‰¹æœ—æ™®æ€»ç»Ÿç‚¹åæ‰¹è¯„åèŒç”Ÿæ‚”æ”¹ä¹‹æ„ï¼Œä»Šæ™šä¸€ç•ªè®²è¯è¢«å¸‚åœºè§£è¯»ä¸ºç¾è”å‚¨æˆ–æš‚åœåŠ æ¯æ­¥ä¼ã€‚ç¾å…ƒæŒ‡æ•°åº”å£°ä¸‹æŒ«ï¼Œç¾è‚¡åŠé‡‘å±è´µé‡‘å±ä»·æ ¼å¤§å¹…ä¸Šæ‰¬ï¼ŒA50è¡¨ç°ä¹Ÿå¹¶ä¸é€Šè‰²å¤ªå¤šã€‚å¯¹æ˜å¤©Aè‚¡æˆ–æœ‰ç§¯æå½±å“ï¼Œåå¼¹æˆ–èƒ½å¾—ä»¥å»¶ç»­ã€‚ [ç»„å›¾å…±2å¼ ]'},...
      ]
    '''

    Match(df_weibo)


